name: Full spikeinterface tests

on:
  pull_request:
    branches: [master]
    types: [synchronize, opened, reopened, ready_for_review]

jobs:
  build-and-test:
    name: Test on (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        # "macos-latest", "windows-latest"
        os: ["ubuntu-latest", ]
    steps:
      - uses: actions/checkout@v2
        with:
          python-version: 3.8
      - name: Get current year-month
        id: date
        run: echo "::set-output name=date::$(date+'%Y-%m')"
      - uses: actions/cache@v2
        id: cache-venv
        with:
          path: ~/test_env
          key: ${{ runner.os }}-venv-${{ hashFiles('**/requirements.txt') }}${{ hashFiles('**/requirements_full.txt') }}-${{ hashFiles('**/requirements_extractors.txt') }}-${{ hashFiles('**/requirements_test.txt') }}-${{ steps.date.outputs.date }}
          restore-keys: |
            ${{ runner.os }}-venv
      - name: Install dependencies
        run: |
          sudo apt update
          # this is for datalad and download testing datasets
          sudo apt install git git-annex
          # needed for correct operation of git/git-annex/DataLad
          git config --global user.email "CI@example.com"
          git config --global user.name "CI Almighty"
          # this is for spyking circus
          sudo apt install mpich libmpich-dev
          # create an environement (better for cahing)
          python -m venv ~/test_env
          source ~/test_env/bin/activate
          python -m pip install --upgrade pip
          pip install setuptools wheel twine
          ##
          #Â force install numpy 1.19 because of numba and also hdbscan !!!!!
          pip install numpy==1.19
          pip install hdbscan --no-cache-dir --no-binary hdbscan
          # Force install neo from github
          pip install git+https://github.com/NeuralEnsemble/python-neo.git
          # TODO remove it after neo release
          ##
          pip install -r requirements.txt
          pip install -r requirements_full.txt
          pip install -r requirements_extractors.txt
          pip install -r requirements_test.txt
          pip install -e .
      - name: pip list
        run: |
          source ~/test_env/bin/activate
          pip list
      - name: Get ephy_testing_data current head hash
        # the key depend on the last comit repo https://gin.g-node.org/NeuralEnsemble/ephy_testing_data.git
        id: vars
        run: |
          echo "::set-output name=HASH_EPHY_DATASET::$(git ls-remote https://gin.g-node.org/NeuralEnsemble/ephy_testing_data.git HEAD | cut -f1)"
      - uses: actions/cache@v2
        id: cache-datasets
        env:
          # the key depend on the last comit repo https://gin.g-node.org/NeuralEnsemble/ephy_testing_data.git
          HASH_EPHY_DATASET: git ls-remote https://gin.g-node.org/NeuralEnsemble/ephy_testing_data.git HEAD | cut -f1
        with:
          path: ~/spikeinterface_datasets
          key: ${{ runner.os }}-datasets-${{ steps.vars.outputs.HASH_EPHY_DATASET }}
          restore-keys: |
            ${{ runner.os }}-datasets
      - name: Test with pytest
        run: |
          source ~/test_env/bin/activate
          pytest

          
# only on ubuntu because of spkyking-circus and mpich
