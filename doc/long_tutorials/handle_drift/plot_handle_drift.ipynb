{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Handle probe drift with spikeinterface NEW\n\nProbe movement is an inevitability when running\n*in vivo* electrophysiology recordings. Motion, caused by physical\nmovement of the probe or the sliding of brain tissue\ndeforming across the probe, can complicate the sorting\nand analysis of units.\n\nSpikeInterface offers a flexible framework to handle motion correction\nas a preprocessing step. In this tutorial we will cover the three main\ndrift-correction algorithms implemented in SpikeInterface\n(**rigid_fast**, **kilosort_like** and **nonrigid_accurate**) with\na focus on running the methods and interpreting the output.\n\nFor more information on the theory and implementation of these methods,\nsee the `motion_correction` section of the documentation and\nthe [kilosort4 page](https://kilosort.readthedocs.io/en/latest/drift.html)\non drift correction. Drift correction may not always work as expected\n(for example, if the probe has a small number of channels), see the\n`When do I need to apply drift correction?`_ section for assessing\ndrift correction output.\n\n## What is probe drift?\n\nThe inserted probe can move from side-to-side (*'x' direction*),\nup-or-down (*'y' direction*) or forwards-or-backwards (*'z' direction*).\nMovement in the 'x' and 'z' direction is harder to model than vertical\ndrift (i.e. along the probe depth), and are not handled by most motion\ncorrection algorithms. Fortunately, vertical drift which is most easily\nhandled is most pronounced as the probe is most likely to move along the path\nof insertion.\n\nVertical drift can come in two forms, *'rigid'* and *'non-rigid'*. Rigid drift\nis drift caused by movement of the entire probe, and the motion is\nsimilar across all channels along the probe depth. In contrast,\nnon-rigid drift is instead caused by local movements of neuronal tissue along the\nprobe, and can selectively affect subsets of channels.\n\n## The drift correction steps\n\nThe easiest way to run drift correction in SpikeInterface is with the\nhigh-level :py:func:`~spikeinterface.preprocessing.correct_motion()` function.\nThis function takes a recording as input and returns a motion-corrected\nrecording object. As with all other preprocessing steps, the correction (in this\ncase interpolation of the data to correct the detected motion) is lazy and applied on-the-fly when data is needed).\n\nThe :py:func:`~spikeinterface.preprocessing.correct_motion()`\nfunction implements motion correction algorithms in a modular way\nwrapping a number of subfunctions that together implement the\nfull drift correction algorithm.\n\nThese drift-correction modules are:\n\n| **1.** ``localize_peaks()`` (detect spikes and localize their position on the probe)\n| **2.** ``select_peaks()`` (optional, select a subset of peaks to use to estimate motion)\n| **3.** ``estimate_motion()`` (estimate motion using the detected spikes)\n| **4.** ``interpolate_motion()`` (perform interpolation on the raw data to account for the estimated drift).\n\nAll these sub-steps have many parameters which dictate the\nspeed and effectiveness of motion correction. As such, ``correct_motion``\nprovides three setting 'presets' which configure the motion correct\nto proceed either as:\n\n* **rigid_fast** - a fast, not particularly accurate correction assuming rigid drift.\n* **kilosort-like** - Mimics what is done in Kilosort.\n* **nonrigid_accurate** - A decentralized drift correction (DREDGE), introduced by the Paninski group.\n\nWhen using motion correction in your analysis, please make sure to\n`cite the appropriate paper for your chosen method<cite-motion-correction>`.\n\n\n**Now, let's dive into running motion correction with these three\nmethods on a simulated dataset.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up and preprocessing the recording\n\nFirst, we will import the modules we will need for this tutorial:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport spikeinterface.full as si\nfrom spikeinterface.generation.drifting_generator import generate_drifting_recording\nfrom spikeinterface.preprocessing.motion import motion_options_preset\nfrom spikeinterface.sortingcomponents.motion_interpolation import correct_motion_on_peaks\nfrom spikeinterface.widgets import plot_peaks_on_probe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will generate a synthetic, drifting recording. This recording will\nhave 100 separate units with firing rates randomly distributed between\n15 and 25 Hz.\n\nWe will create a zigzag drift pattern on the recording, starting at\n100 seconds and with a peak-to-peak period of 100 seconds (so we will\nhave 9 zigzags through our recording). We also add some non-linearity\nto the imposed motion.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial can take a long time to run with the default arguments.\n    If you would like to run this locally, you may want to edit ``num_units``\n    and ``duration`` to smaller values (e.g. 25 and 100 respectively).\n\n    Also note, the below code uses multiprocessing. If you are on Windows, you may\n    need to place the code within a  ``if __name__ == \"__main__\":`` block.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_units = 200  # 250 still too many I think!\nduration = 1000\n\n_, raw_recording, _ = generate_drifting_recording(\n    num_units=num_units,\n    duration=duration,\n    generate_sorting_kwargs=dict(firing_rates=(15, 25), refractory_period_ms=4.0),\n    seed=42,\n    generate_displacement_vector_kwargs=dict(motion_list=[\n            dict(\n                drift_mode=\"zigzag\",\n                non_rigid_gradient=0.01,\n                t_start_drift=int(duration/10),\n                t_end_drift=None,\n                period_s=int(duration/10),\n            ),\n        ],\n    )\n)\nprint(raw_recording)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before performing motion correction, we will **preprocess** the recording\nwith a bandpass filter and a common median reference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filtered_recording = si.bandpass_filter(raw_recording, freq_min=300.0, freq_max=6000.0)\npreprocessed_recording = si.common_reference(filtered_recording, reference=\"global\", operator=\"median\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>It is better to not whiten the recording before motion estimation, as this\n    will give a better estimate of the peak locations. Whitening should\n    be performed after motion correction.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run motion correction with one function!\n\nCorrecting for drift is easy! You just need to run a single function.\nWe will now run motion correction on our recording using the three\npresets described above - **rigid_fast**, **kilosort_like** and\n**nonrigid_accurate**.\n\nWe can run these presents with the ``preset`` argument of\n:py:func:`~spikeinterface.preprocessing.correct_motion()`. Under the\nhood, the presets define a set of parameters by set how to run the\n4 submodules that make up motion correction (described above).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(motion_options_preset[\"kilosort_like\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets run motion correction with our three presets. We will\nset the ``job_kwargs`` to parallelize the job over a number of CPU cores\u2014motion\ncorrection is computationally intensive and will run faster with parallelization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "presets_to_run = (\"rigid_fast\", \"kilosort_like\", \"nonrigid_accurate\")\n\njob_kwargs = dict(n_jobs=40, chunk_duration=\"1s\", progress_bar=True)\n\nresults = {preset: {} for preset in presets_to_run}\nfor preset in presets_to_run:\n\n    corrected_recording, motion_info = si.correct_motion(\n        preprocessed_recording, preset=preset,  output_motion_info=True, **job_kwargs\n    )\n    results[preset][\"motion_info\"] = motion_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. seealso::\n   It is often very useful to save ``motion_info`` to a\n   file, so it can be loaded and visualized later. This can be done by setting\n   the ``folder`` argument of\n   :py:func:`~spikeinterface.preprocessing.correct_motion()` to a path to write\n   all motion outputs to. The ``motion_info`` can be loaded back with\n   ``si.load_motion_info``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the results\n\nNext, let's plot the results of our motion estimation using the ``plot_motion_info()``\nfunction. The plot contains 4 panels, on the x-axis of all plots we have\nthe (binned time). The plots display:\n  * **top left:** The estimated peak depth for every detected peak.\n  * **top right:** The estimated peak depths after motion correction.\n  * **bottom left:** The average motion vector across depths and all motion across spatial depths (for non-rigid estimation).\n  * **bottom right:** if motion correction is non-rigid, the motion vector across depths is plotted as a map, with the color code representing the motion in micrometers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for preset in presets_to_run:\n\n    fig = plt.figure(figsize=(7, 7))\n\n    si.plot_motion_info(\n        results[preset][\"motion_info\"],\n        recording=corrected_recording,  # the recording is only used to get the real times\n        figure=fig,\n        depth_lim=(400, 600),\n        color_amplitude=True,\n        amplitude_cmap=\"inferno\",\n        scatter_decimate=10,            # Only plot every 10th peak\n    )\n    fig.suptitle(f\"{preset=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These plots are quite complicated, so it is worth covering them in detail.\nFor every detected spike in our recording, we first estimate\nits depth (first panel) using a method from\n:py:func:`~spikeinterface.postprocessing.compute_unit_locations()`.\n\nThen, the probe motion is estimated and the location of the\nspikes are adjusted to account for the motion (second panel).\n\nThe motion estimation produces\na measure of how much and in what direction the probe is moving at any given\ntime bin (third panel). For non-rigid motion correction, the probe is divided\ninto subsections - the motion vectors displayed are per subsection (i.e. per\n'binned spatial depth') as well as the average.\n\nOn the fourth panel, we see a\nmore detailed representation of the motion vectors. We can see the motion plotted\nas a heatmap at each binned spatial depth across all time bins. It captures\nthe zigzag pattern (alternating light and dark colors) of the injected motion.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few comments on the figures:\n  * The preset **'rigid_fast'** has only one motion vector for the entire probe because it is a 'rigid' case.\n    The motion amplitude is globally underestimated because it averages across depths.\n    However, the corrected peaks are flatter than the non-corrected ones, so the job is partially done.\n    The big jump at 600s when the probe start moving is recovered quite well.\n  * The preset **kilosort_like** gives better results because it is a non-rigid case.\n    The motion vector is computed for different depths.\n    The corrected peak locations are flatter than the rigid case.\n    The motion vector map is still a bit noisy at some depths (e.g around 1000um).\n  * The preset **nonrigid_accurate** seems to give the best results on this recording.\n    The motion vector seems less noisy globally, but it is not 'perfect' (see at the top of the probe 3200um to 3800um).\n    Also note that in the first part of the recording before the imposed motion (0-600s) we clearly have a non-rigid motion:\n    the upper part of the probe (2000-3000um) experience some drifts, but the lower part (0-1000um) is relatively stable.\n    The method defined by this preset is able to capture this.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correcting Peak Locations after Motion Correction\n\nThe result of motion correction can be applied to the data in two ways.\nThe first is by interpolating the raw traces to correct for the estimated drift.\nThis changes the data in the\nrecording by shifting the signal across channels, and is given in the\n`corrected_recording` output from :py:func:`~spikeinterface.preprocessing.correct_motion()`.\nThis is useful in most cases, for continuing\nwith preprocessing and sorting with the corrected recording.\n\nThe second way is to apply the results of motion correction directly\nto the ``peak_locations`` object. If you are not familiar with\nSpikeInterface's ``peak`` and ``peak_locations`` objects,\nthese are explored further in the below dropdown.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. dropdown:: 'Peaks' and 'Peak Locations' in SpikeInterface\n\n  Information about detected spikes is represented in\n  SpikeInterface's ``peaks`` and ``peak_locations`` objects. The\n  ``peaks`` object is an array for containing the\n  sample index, channel index (where its signal\n  is strongest), amplitude and recording segment index for every detected spike\n  in the dataset. It is created by the\n  :py:func:`~spikeinterface.sortingcomponents.peak_detection.detect_peaks()`\n  function.\n\n  The ``peak_locations`` is a partner object to the ``peaks`` object,\n  and contains the estimated location (``\"x\"``, ``\"y\"``) of the spike. For every spike in\n  ``peaks`` there is a corresponding location in ``peak_locations``.\n  The peak locations is estimated using the\n  :py:func:`~spikeinterface.sortingcomponents.peak_localization.localise_peaks()`\n  function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The other way to apply the motion correction is to the ``peaks`` and\n``peaks_location`` objects directly. This is done using the function\n``correct_motion_on_peaks()``. Given a set of peaks, peak locations and\nthe ``motion`` object output from :py:func:`~spikeinterface.preprocessing.correct_motion()`,\nit will shift the location of the peaks according to the motion estimate, outputting a new\n``peak_locations`` object. This is done to plot the peak locations in\nthe next section.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Note that the ``peak_locations`` output by\n   :py:func:`~spikeinterface.preprocessing.correct_motion()`\n   (in the ``motion_info`` object) is the original (uncorrected) peak locations.\n   To get the corrected peak locations, ``correct_motion_on_peaks()`` must be used!</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for preset in presets_to_run:\n\n    motion_info = results[preset][\"motion_info\"]\n\n    peaks = motion_info[\"peaks\"]\n\n    original_peak_locations = motion_info[\"peak_locations\"]\n\n    corrected_peak_locations = correct_motion_on_peaks(peaks, original_peak_locations, motion_info['motion'], corrected_recording)\n\n    widget = plot_peaks_on_probe(corrected_recording, [peaks, peaks], [original_peak_locations, corrected_peak_locations], ylim=(300,600))\n    widget.figure.suptitle(preset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing the  Run Times\n\nThe different methods also have different speeds, the 'nonrigid_accurate'\nrequires more computation time, in particular at the ``estimate_motion`` phase,\nas seen in the run times:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for preset in presets_to_run:\n    print(preset)\n    print(results[preset][\"motion_info\"][\"run_times\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When do I need to apply drift correction?\n\nDrift correction may not always be necessary for your data, for\nexample, for example when there is not much drift in the data to begin with.\nFurther, in some cases (e.g. when the probe has a smaller number of channels,\ne.g. 64 or less) the drift correction algorithms may not perfect well.\n\nTo check whether drift correction is required and how it is performing,\nit is necessary to run drift correction as above and then check the output plots.\nIn the below example, the 'Peak depth' plot shows minimal drift in the peak position.\nIn this example, it does not look like drift correction is that necessary. Further,\nbecause there are only 16 channels in this recording, the drift correction is failing.\nThe 'Correct Peak Depth' as erroenously shifted peaks to the wrong position, spreading\nthem across the probe. In this instance, drift correction could be skipped.\n\n<img src=\"file://../../images/no-drift-example.png\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nThat's it for our tour of motion correction in\nSpikeInterface. Remember that correcting motion makes some\nassumptions on your data (e.g. number of channels, noise in the recording)\u2014always\nplot the motion correction information for your\nrecordings, to make sure the correction is behaving as expected!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
