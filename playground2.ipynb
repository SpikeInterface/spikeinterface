{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground2: Kilosort + Template Metrics\n",
    "\n",
    "This notebook:\n",
    "1. Runs Kilosort4 (if not already done)\n",
    "2. Loads data and sorting results\n",
    "3. Computes template metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import spikeinterface.full as si\n",
    "\n",
    "print(f\"SpikeInterface version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_folder = Path(\"/Users/jf5479/Downloads/AL031_2019-12-02\")\n",
    "bin_file = data_folder / \"AL031_2019-12-02_bank1_NatIm_g0_t0_bc_decompressed.imec0.ap.bin\"\n",
    "meta_file = data_folder / \"AL031_2019-12-02_bank1_NatIm_g0_t0.imec0.ap.meta\"\n",
    "\n",
    "# Output paths\n",
    "output_folder = data_folder / \"spikeinterface_output\"\n",
    "kilosort_output = output_folder / \"kilosort4_output\"\n",
    "analyzer_folder = output_folder / \"sorting_analyzer\"\n",
    "\n",
    "# Job kwargs for parallel processing\n",
    "job_kwargs = dict(n_jobs=-1, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "print(f\"Data folder: {data_folder}\")\n",
    "print(f\"Bin file exists: {bin_file.exists()}\")\n",
    "print(f\"Meta file exists: {meta_file.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# The bin and meta files have different names, so we need to load manually\nfrom neo.rawio.spikeglxrawio import read_meta_file\nfrom spikeinterface.extractors.cbin_ibl import extract_stream_info\nfrom spikeinterface.extractors.neuropixels_utils import get_neuropixels_sample_shifts\nimport probeinterface\n\n# Read meta file\nmeta = read_meta_file(meta_file)\ninfo = extract_stream_info(meta_file, meta)\n\n# Get parameters\nnum_channels = info[\"num_chan\"]\nsampling_frequency = info[\"sampling_rate\"]\nchannel_gains = info[\"channel_gains\"]\nchannel_offsets = info[\"channel_offsets\"]\nchannel_ids = info[\"channel_names\"]\n\n# Remove sync channel (last channel)\nnum_channels_no_sync = num_channels - 1\nchannel_gains_no_sync = channel_gains[:-1]\nchannel_offsets_no_sync = channel_offsets[:-1]\nchannel_ids_no_sync = channel_ids[:-1]\n\nprint(f\"Sampling frequency: {sampling_frequency} Hz\")\nprint(f\"Number of channels (without sync): {num_channels_no_sync}\")\n\n# Load as binary recording\nrecording = si.read_binary(\n    file_paths=bin_file,\n    sampling_frequency=sampling_frequency,\n    num_channels=num_channels,  # Include sync for reading, will remove later\n    dtype=\"int16\",\n)\n\n# Remove sync channel using select_channels\nrecording = recording.select_channels(channel_ids=recording.channel_ids[:-1])\n\n# Set gains and offsets\nrecording.set_channel_gains(channel_gains_no_sync)\nrecording.set_channel_offsets(channel_offsets_no_sync)\n\n# Load and attach probe from meta file\nprobe = probeinterface.read_spikeglx(meta_file)\nrecording = recording.set_probe(probe)\n\n# Set inter_sample_shift property for phase correction (needed for Neuropixels)\nptype = probe.annotations.get(\"probe_type\", 0)\nif ptype in [21, 24]:  # NP2.0\n    num_channels_per_adc = 16\nelse:  # NP1.0\n    num_channels_per_adc = 12\n\nsample_shifts = get_neuropixels_sample_shifts(recording.get_num_channels(), num_channels_per_adc)\nrecording.set_property(\"inter_sample_shift\", sample_shifts)\n\nprint(f\"Loaded recording: {recording}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Duration: {recording.get_total_duration():.2f} s\")\nprint(f\"Probe: {recording.get_probe()}\")\nrecording"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# High-pass filter\nrec_filtered = si.highpass_filter(recording, freq_min=300.0)\n\n# Detect and remove bad channels\nbad_channel_ids, channel_labels = si.detect_bad_channels(rec_filtered)\nprint(f\"Bad channels detected: {len(bad_channel_ids)}\")\nif len(bad_channel_ids) > 0:\n    print(f\"Bad channel IDs: {bad_channel_ids}\")\n    rec_clean = rec_filtered.remove_channels(bad_channel_ids)\nelse:\n    rec_clean = rec_filtered\n\n# Skip phase_shift - Kilosort4 handles this internally\n# Common median reference\nrec_preprocessed = si.common_reference(rec_clean, operator=\"median\", reference=\"global\")\n\nprint(f\"Preprocessed recording: {rec_preprocessed}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Kilosort4 (if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if Kilosort output already exists\nif kilosort_output.exists() and (kilosort_output / \"spike_times.npy\").exists():\n    print(f\"Kilosort output already exists at: {kilosort_output}\")\n    print(\"Loading existing sorting results...\")\n    sorting = si.read_sorter_folder(kilosort_output)\nelse:\n    print(f\"Running Kilosort4, output will be saved to: {kilosort_output}\")\n    print(f\"Installed sorters: {si.installed_sorters()}\")\n\n    # Run Kilosort4\n    sorting = si.run_sorter(\n        sorter_name=\"kilosort4\",\n        recording=rec_preprocessed,\n        folder=kilosort_output,\n        verbose=True,\n        remove_existing_folder=True,  # Remove any failed previous attempts\n    )\n    print(\"Kilosort4 completed!\")\n\nprint(f\"Sorting result: {sorting}\")\nprint(f\"Number of units: {len(sorting.unit_ids)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create SortingAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if analyzer already exists\n",
    "if analyzer_folder.exists():\n",
    "    print(f\"Loading existing analyzer from: {analyzer_folder}\")\n",
    "    analyzer = si.load_sorting_analyzer(analyzer_folder)\n",
    "else:\n",
    "    print(f\"Creating new analyzer at: {analyzer_folder}\")\n",
    "    analyzer = si.create_sorting_analyzer(\n",
    "        sorting=sorting,\n",
    "        recording=rec_preprocessed,\n",
    "        sparse=True,\n",
    "        format=\"binary_folder\",\n",
    "        folder=analyzer_folder,\n",
    "    )\n",
    "\n",
    "analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Extensions for Template Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random spikes selection\n",
    "if not analyzer.has_extension(\"random_spikes\"):\n",
    "    print(\"Computing random_spikes...\")\n",
    "    analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveforms\n",
    "if not analyzer.has_extension(\"waveforms\"):\n",
    "    print(\"Computing waveforms...\")\n",
    "    analyzer.compute(\"waveforms\", ms_before=1.5, ms_after=2.0, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates\n",
    "if not analyzer.has_extension(\"templates\"):\n",
    "    print(\"Computing templates...\")\n",
    "    analyzer.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise levels\n",
    "if not analyzer.has_extension(\"noise_levels\"):\n",
    "    print(\"Computing noise_levels...\")\n",
    "    analyzer.compute(\"noise_levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Template Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute template metrics with multi-channel metrics included\n",
    "if not analyzer.has_extension(\"template_metrics\"):\n",
    "    print(\"Computing template_metrics...\")\n",
    "    analyzer.compute(\n",
    "        \"template_metrics\",\n",
    "        include_multi_channel_metrics=True,\n",
    "    )\n",
    "\n",
    "# Get the metrics as a DataFrame\n",
    "template_metrics = analyzer.get_extension(\"template_metrics\").get_data()\n",
    "template_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Quality Metrics (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike amplitudes\n",
    "if not analyzer.has_extension(\"spike_amplitudes\"):\n",
    "    print(\"Computing spike_amplitudes...\")\n",
    "    analyzer.compute(\"spike_amplitudes\", **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlograms\n",
    "if not analyzer.has_extension(\"correlograms\"):\n",
    "    print(\"Computing correlograms...\")\n",
    "    analyzer.compute(\"correlograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics\n",
    "if not analyzer.has_extension(\"quality_metrics\"):\n",
    "    print(\"Computing quality_metrics...\")\n",
    "    analyzer.compute(\"quality_metrics\")\n",
    "\n",
    "quality_metrics = analyzer.get_extension(\"quality_metrics\").get_data()\n",
    "quality_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total units: {len(sorting.unit_ids)}\")\n",
    "print(f\"Analyzer saved to: {analyzer_folder}\")\n",
    "print(f\"\\nAvailable extensions:\")\n",
    "for ext_name in analyzer.get_loaded_extension_names():\n",
    "    print(f\"  - {ext_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine metrics\n",
    "combined_metrics = template_metrics.join(quality_metrics, how=\"outer\")\n",
    "combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "metrics_csv = output_folder / \"combined_metrics.csv\"\n",
    "combined_metrics.to_csv(metrics_csv)\n",
    "print(f\"Metrics saved to: {metrics_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}