from __future__ import annotations

from pathlib import Path
import shutil

from spikeinterface.core.job_tools import _shared_job_kwargs_doc, fix_job_kwargs
import spikeinterface.widgets as sw
from spikeinterface.core import get_template_extremum_channel, get_template_extremum_amplitude
from spikeinterface.postprocessing import compute_correlograms


def export_report(
    sorting_analyzer,
    output_folder,
    remove_if_exists=False,
    format="png",
    show_figures=False,
    peak_sign="neg",
    force_computation=False,
    **job_kwargs,
):
    """
    Exports a SI spike sorting report. The report includes summary figures of the spike sorting output
    (e.g. amplitude distributions, unit localization and depth VS amplitude) as well as unit-specific reports,
    that include waveforms, templates, template maps, ISI distributions, and more.


    Parameters
    ----------
    sorting_analyzer : SortingAnalyzer
        A SortingAnalyzer object
    output_folder : str
        The output folder where the report files are saved
    remove_if_exists : bool, default: False
        If True and the output folder exists, it is removed
    format : str, default: "png"
        The output figure format (any format handled by matplotlib)
    peak_sign : "neg" or "pos", default: "neg"
        used to compute amplitudes and metrics
    show_figures : bool, default: False
        If True, figures are shown. If False, figures are closed after saving
    force_computation :  bool, default: False
        Force or not some heavy computaion before exporting
    {}
    """
    import pandas as pd
    import matplotlib.pyplot as plt

    job_kwargs = fix_job_kwargs(job_kwargs)
    sorting = sorting_analyzer.sorting
    unit_ids = sorting_analyzer.unit_ids

    # load or compute spike_amplitudes
    if sorting_analyzer.has_extension("spike_amplitudes"):
        spike_amplitudes = sorting_analyzer.get_extension("spike_amplitudes").get_data(outputs="by_unit")
    elif force_computation:
        sorting_analyzer.compute("spike_amplitudes", **job_kwargs)
        spike_amplitudes = sorting_analyzer.get_extension("spike_amplitudes").get_data(outputs="by_unit")
    else:
        spike_amplitudes = None
        print(
            "export_report(): spike_amplitudes will not be exported. Use sorting_analyzer.compute('spike_amplitudes') if you want to include them."
        )

    # load or compute quality_metrics
    if sorting_analyzer.has_extension("quality_metrics"):
        metrics = sorting_analyzer.get_extension("quality_metrics").get_data()
    elif force_computation:
        sorting_analyzer.compute("quality_metrics")
        metrics = sorting_analyzer.get_extension("quality_metrics").get_data()
    else:
        metrics = None
        print(
            "export_report(): quality metrics will not be exported. Use sorting_analyzer.compute('quality_metrics') if you want to include them."
        )

    # load or compute correlograms
    if sorting_analyzer.has_extension("correlograms"):
        correlograms, bins = sorting_analyzer.get_extension("correlograms").get_data()
    elif force_computation:
        correlograms, bins = compute_correlograms(sorting_analyzer, window_ms=100.0, bin_ms=1.0)
    else:
        correlograms = None
        print(
            "export_report(): correlograms will not be exported. Use sorting_anlyzer.compute('correlograms') if you want to include them."
        )

    # pre-compute unit locations if not done
    if not sorting_analyzer.has_extension("unit_locations"):
        sorting_analyzer.compute("unit_locations")

    output_folder = Path(output_folder).absolute()
    if output_folder.is_dir():
        if remove_if_exists:
            shutil.rmtree(output_folder)
        else:
            raise FileExistsError(f"{output_folder} already exists")
    output_folder.mkdir(parents=True, exist_ok=True)

    # unit list
    units = pd.DataFrame(index=unit_ids)  # Â , columns=['max_on_channel_id', 'amplitude'])
    units.index.name = "unit_id"
    units["max_on_channel_id"] = pd.Series(
        get_template_extremum_channel(sorting_analyzer, peak_sign=peak_sign, outputs="id")
    )
    units["amplitude"] = pd.Series(get_template_extremum_amplitude(sorting_analyzer, peak_sign=peak_sign))
    units.to_csv(output_folder / "unit list.csv", sep="\t")

    unit_colors = sw.get_unit_colors(sorting)

    # global figures
    fig = plt.figure(figsize=(20, 10))
    w = sw.plot_unit_locations(sorting_analyzer, figure=fig, unit_colors=unit_colors)
    fig.savefig(output_folder / f"unit_localization.{format}")
    if not show_figures:
        plt.close(fig)

    fig, ax = plt.subplots(figsize=(20, 10))
    sw.plot_unit_depths(sorting_analyzer, ax=ax, unit_colors=unit_colors)
    fig.savefig(output_folder / f"unit_depths.{format}")
    if not show_figures:
        plt.close(fig)

    if spike_amplitudes and len(unit_ids) < 100:
        fig = plt.figure(figsize=(20, 10))
        sw.plot_all_amplitudes_distributions(sorting_analyzer, figure=fig, unit_colors=unit_colors)
        fig.savefig(output_folder / f"amplitudes_distribution.{format}")
        if not show_figures:
            plt.close(fig)

    if metrics is not None:
        metrics.to_csv(output_folder / "quality metrics.csv")

    # units
    units_folder = output_folder / "units"
    units_folder.mkdir()

    for unit_id in unit_ids:
        fig = plt.figure(
            constrained_layout=False,
            figsize=(15, 7),
        )
        sw.plot_unit_summary(sorting_analyzer, unit_id, figure=fig)
        fig.suptitle(f"unit {unit_id}")
        fig.savefig(units_folder / f"{unit_id}.{format}")
        if not show_figures:
            plt.close(fig)


export_report.__doc__ = export_report.__doc__.format(_shared_job_kwargs_doc)
